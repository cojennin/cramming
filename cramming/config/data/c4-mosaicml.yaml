# This would be a slice of C4
is_mosaic: true
name: c4-subset

train_loader:
    name: text
    dataset:
      local: /tmp/c4
      remote: 's3://mosaicml-internal-dataset-c4/mds/2/'
      split: train
      tokenizer_name: bert-base-uncased
      max_seq_len: 128
      shuffle: true
      mlm_probability: 0.30
    drop_last: true
    num_workers: 8

#
# Preprocessing
normalizer:
  force_lowercase: True
  strip_accents: True
  force_english_keyboard: True
  whitespace_escape: False

vocab_size: 30522 # 2^15

# Dataset Formation
seq_length: 128
include_cls_token_in_corpus: False
include_sep_token_in_corpus: True
use_type_ids: False
max_entries_in_raw_dataset: 25e6 # Select only this many examples from the dataset # 20e6 are ok if all are chosen. Oversample if filtering
max_seq_in_tokenized_dataset: 85e6 # Select only this many tokenized sequences.
# max_seq_in_tokenized_dataset should be just slightly more than budget * 60 * 60 * expected tokens/sec for the single epoch of training

# Data Cleaning:
named_entity_simplification: False
remove_whitespaces: False
remove_trash: True
trash_cutoff: 0.25
deduplicate_entries: True
deduplication_threshold: 75

# Data Order:
ordering: sentence-length-curriculum # could be a curriculum
